\documentclass[twoside]{article}
\usepackage{amsmath,amssymb,amsthm,graphicx}
\usepackage{epsfig}
\usepackage[authoryear]{natbib}
\usepackage{enumitem}
\usepackage{mathabx}
\input{stat-macros}

\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\Large \hfill #2  \hfill}  }
       \vspace{6mm}
       \hbox to 6.28in { {\Large \hfill #1  \hfill}  }
       \vspace{6mm}
       \hbox to 6.28in { {\it Author: #3 \hfill Date: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{#1}{#1}
   \vspace*{4mm}
}

% Local Macros Put your favorite macros here that don't appear in
% stat-macros.tex.  We can eventually incorporate them into
% stat-macros.tex if they're of general use.

\begin{document}

\lecture{Learning a non-parametric approximate posterior for NNs}{}{}{November 23, 2017}

\section{the GAN-way}

The motivation behind this is to learn a model for the approximate posterior $q(\mathcal{W})$ of a deep neural network. The learnt model can then be utilised for sampling different weights at test-time for robust predictions.

GANs are a way of approximating some unknown target distribution, where you only need to be able to sample from the approximating distribution and take gradients of expectations through monte carlo estimates.

Let's say the target distribution is the predictive distribution from a Bayesin-NN, $p_{\text{exact}}(y|x)$. Our aim is to define a generative process that samples from $p_{\text{approx}}(y|x)$. Suppose we have a model $t_\theta$ which defines the approximate posterior, $q(W)$, i.e. if $\{\epsilon_i\}_{i=1}^M \sim p(\epsilon)$, then \{$W_i | W_i = t_\theta(\epsilon_i)\}_{i=1}^M\sim q(W)$. Subsequently, one could obtain a sample $y_i \sim p_{\text{approx}}(y|x)$ as $f(x,t_\theta(\epsilon_i))$. 

Now suppose we have a discriminator $D_\psi$, to compare the two distributions, $p_{\text{approx}}$ and $p_{\text{exact}}$. We can thus write a min-max objective function as,


\begin{align}
\text{min}_\theta\ \text{max}_\psi\ \mathbb{E}_{y\sim p_{\text{exact}}(y|x)}[\log D_\psi(y)] + \mathbb{E}_{y\sim p_{\text{approx}}(y|x)}[\log \left( 1-D_\psi(y) \right) ]\\
\end{align}

The ``desired'' second term needs to be reformulated in temrs of $p(\epsilon)$. In short any expectation over $p_{\text{approx}}(y|x)$ needs to be written as an expectation over $p(\epsilon)$), but are they equivalent? or can we sample moments of $W$ and then integrate under some gaussian assumptions?

\begin{align}
\mathbb{E}_{\epsilon\sim p_{\epsilon}}[\log \left( 1-D_\psi(f(t_\theta(\epsilon))) \right) ]\\
\end{align}


The problem is that while computing $y_i$ as $f(x,t_\theta(\epsilon_i))$, we are not marginalising over the weights. 
% It could take as input a vector $x$ along with a set of noise-samples $\{\epsilon_i\}_{i=1}^M \sim p(\epsilon)$ and outputs samples $y \sim p_{\text{approx}}(y|x)$. This can be thought of as learning a prametric model $t_\theta$ for the approximate posterior,

\begin{align}
p_{\text{approx}}(y|x) &= \int p(y|x,W)q(W) dW\\
& = \int p(y|x,t_\theta(\epsilon))p(\epsilon)d\epsilon\\
&\approx \frac{1}{M} \sum_{m=1}^{M} p(y|x,t_\theta(\epsilon_m)), \ \epsilon \sim p(\epsilon)
\end{align}

To make this more clear, for every $\epsilon_i$ what you get is a paired-sample $(y_i,W_i)$ from the joint distribution $p(y_i.W_i|x)$. Thus, marginalisation with respect to $W$ is a must here. Otherwise, your approximation model for posterior is a dirac on $W_i$.


\section{the VAE-way - I}

Suppose there is a parametric model $g_\theta$ which takes as input noise and outputs the mean and variance corresponding to the approximate posterior. Now if you make the same assumption as PBP, that the output of the network admit a Gaussian distribution then you can sucessfully represent a sample from the distribution $p(y|x)$. One can then learn this model using either the ELBO, or the GAN-way.

Assumtions: It is assumed that the approximate posterior factors over weight, conditioned on the latent space, which is more complex then a naively factored distribution of weights. 

\begin{align}
q(W) = \prod_{j=1}^Lq(W_j) = \prod_{j=1}^L \mathcal{N}(m_j,v_j)
\end{align}

\begin{align}
q(W) &= \int q(W,z)dz\\
&= \int q(W|z)p(z)dz\\
&= \int \left(\prod_{j=1}^Lq(W_j|z)\right)p(z)dz\\
&= \int \left(\prod_{j=1}^L\mathcal{N}(m_j,v_j)\right)\mathcal{N}(m_z,v_z)dz\\
\end{align}

\begin{align}
\epsilon_i \sim p(\epsilon) \rightarrow \{m_{i,j},v_{i,j}\}_{j=1}^{L} = t_\theta(\epsilon_i) \rightarrow q\\
\end{align}

Limitations: 1. the approximate posterior in this model is not as expressive as hypernetworks'. 2. This wouldn't scale to high dimensional datasets if you make such big assumptions like normally distributed outputs. 

\section{the VAE-way - II}

This idea attempts to learm a latent space of the approximate posterior. I will discuss how this differs from Hypernetworks and what's the advantage of this modelling. The merits are clear if you see the generative process for getting samples of $q(W)$ differs in the two approaches. 

\begin{align}
\epsilon_i \sim p(\epsilon) \rightarrow W_i = t_\theta(\epsilon_i)\\
m,v \rightarrow z_i \sim \mathcal{N}(m,v) \rightarrow W_i = t_\theta(z_i)
\end{align}

\begin{itemize}
  \item Is this equivalent to putting hierarchical priors in the Hypernetwork formulation? 
\end{itemize}

Both the formilations I and II can be interpretted as learning a latent space for approximate posterior. 

 
\section{Learning a parametric model for transition distributions in MCMC}

In a Markov Chain MC sampler, there is a ditribution $T(W^{t}|W^{t-1})$ which captures the transitions. The stationary distribution of this chain corresponds to the required distribution for which the sampler is developed which in our case is the approximate posterior $q(W)$. 

\begin{align}
q(W^t) = \int T(W^{t}|W^{t-1})q(W^{t-1})dW
\end{align}

Is it possible to learn a NN-model for this transition distribution? Something similar to ``Neural Adaptive Sequential Monte Carlo [Gu et. al. 2014]''


% section distribution_perspective (end)

%\bibliographystyle{unsrt}
%\bibliography{citations}
% section siamese_networks (end)
\end{document}



