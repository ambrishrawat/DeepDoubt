\documentclass[twoside]{article}
\usepackage{amsmath,amssymb,amsthm,graphicx}
\usepackage{epsfig}
\usepackage[authoryear]{natbib}
\usepackage{enumitem}
\usepackage{mathabx}
\input{stat-macros}

\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\Large \hfill #2  \hfill}  }
       \vspace{6mm}
       \hbox to 6.28in { {\Large \hfill #1  \hfill}  }
       \vspace{6mm}
       \hbox to 6.28in { {\it Author: #3 \hfill Date: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{#1}{#1}
   \vspace*{4mm}
}

% Local Macros Put your favorite macros here that don't appear in
% stat-macros.tex.  We can eventually incorporate them into
% stat-macros.tex if they're of general use.

\begin{document}

\lecture{Learning an approximate posterior for NNs}{}{}{November 23, 2017}

\section{the GAN-way}

The motivation behind this is to learn a model for the approximate posterior $q(\mathcal{W})$ of a deep neural network. The learnt model can then be utilised for sampling different weights at test-time for robust predictions.

GANs are a way of approximating some unknown distribution, where you only need to be able to sample from the approximating distribution and take gradients of expectations through monte carlo estimates.

Let's say our target distribution is $p_{\text{exact}}(y|x)$. Now suppose we have a generator model which approximates this. It takes as input a vector $x$ and outputs $p_{\text{approx}}(y|x)$. However it arrives at this in the following fashion,

\begin{align}
p_{\text{approx}}(y|x) &= \int p(y|x,W)q(W) dW\\
& = \int p(y|x,t(\epsilon))p(\epsilon)d\epsilon\\
&\approx \frac{1}{M} \sum_{m=1}^{M} p(y|x,t(\epsilon_m)), \ \epsilon \sim p(\epsilon)
\end{align}

Or in short any expectation over $p_{\text{approx}}(y|x)$ can be reformulated as an expectation over $p(\epsilon)$.

Now suppose we have a discriminator to compare the two distributions. We can thus write our objective function as,

\begin{align}
\text{min}_t\ \text{max}_D\ \mathbb{E}[\log D(p_{\text{exact}}(y|x))] + \mathbb{E}[\log \left( 1-D(p_{\text{approx}}(y|x) \right) ]\\
\end{align}


\section{Learning a parametric model MCMC proposal distribution}

the ELBO for a NN model with weights W, can be written as,

\begin{align}
\mathbb{E}_{q(W)}[\log p(y|x,W) -\log q(W) + \log p(W)]
\end{align}



% section distribution_perspective (end)

%\bibliographystyle{unsrt}
%\bibliography{citations}
% section siamese_networks (end)
\end{document}



