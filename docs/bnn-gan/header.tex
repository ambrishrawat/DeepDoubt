\documentclass[twoside]{article}
\usepackage{amsmath,amssymb,amsthm,graphicx}
\usepackage{epsfig}
\usepackage[authoryear]{natbib}
\usepackage{enumitem}
\usepackage{mathabx}
\input{stat-macros}

\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\Large \hfill #2  \hfill}  }
       \vspace{6mm}
       \hbox to 6.28in { {\Large \hfill #1  \hfill}  }
       \vspace{6mm}
       \hbox to 6.28in { {\it Author: #3 \hfill Date: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{#1}{#1}
   \vspace*{4mm}
}

% Local Macros Put your favorite macros here that don't appear in
% stat-macros.tex.  We can eventually incorporate them into
% stat-macros.tex if they're of general use.

\begin{document}

\lecture{Learning a implicit approximate posterior for NNs}{}{}{November 23, 2017}

The motivation here is to learn a model for the approximate posterior $q(\mathcal{W})$ of a deep neural network. The learnt model can then be utilised for well caliberated predictions with uncertainty estimates. There are various approaches to get a feedback for learning in a machine learning model - cost function, marginal likelihood or its lower bound (ELBO), or divergence measures between two distributions (GANs and the related family of models). 

Let's say the task is to model $p(y|x)$ which is modelled through a deterministic (continous, differentiable), function $f_\theta$ (often a Neural Network). For regression $p(y|x)$ is modelled as a Gaussian, $\mathcal{N}(f_\theta(x),\gamma^{-1})$. Similarly, for classification, $p(y|x)$ is modelled as a categorical distribution which is realised through a softmax operation on the outputs of the last layer. The densities for $p(y|x)$ can be evaluated accordingly.

\paragraph{Other Tasks}
Even unsupervised tasks can be modelled this way or latent variable models like Matrix factorisation or generative models. All these tasks can be visualised as a task-layer. The objective of our modelling will be to perform these tasks probabilistically. 

\section{Adversarial Learning}

GANs are a way of approximating some unknown target distribution, where you only need to be able to sample from the approximating distribution and take gradients of expectations through monte carlo estimates. The central game-theoretic idea behind GANs is the objective function which in its generality can be summarised as,

\begin{align}
\mathbb{E}_{y\sim p_{\text{data}}}[\log D(y)] + \mathbb{E}_{y\sim p_{\text{model}}}[\log\left(1-D(y)\right)]
\label{eq:gan-obj}
\end{align}

This formulation can effectively substitute for most loss function in machine learning modelling. The supervised learning tasks of classification and regression model $p(y|x)$ and thus one could learn the models using GAN-objective function as loss instead of \textit{cross-entropy} or \textit{mean-squared loss}. \footnote{It is effectively JSD as opposed to KL diveregnce minimisation between the two distributions. Add to that, the JSD is adversarilly learnt by simultaneously learning a discriminator.} It is important to note that in these formulations, the game might be highly imbalanced between the discrimnator process and the generator process. 


\paragraph{Learning without posterior (something is wrong here). }
Let's say the target distribution is $p_{\text{data}}(y|x)$. Our aim is to define a generative process that samples from $p_{\text{model}}(y|x)$ which could be obtained as say, the predictive distribution of a Stochastic Neural Network. Bayesian Neural Networks are one way to model stochastic nets. Suppose the task of classification/regression is modelled as a functional transform $y=f(x,W)$. Now suppose we have a discriminator $D_\psi$, to compare the samples from two distributions, $p_{\text{model}}$ and $p_{\text{data}}$. We can thus write a min-max objective function as,

\begin{align}
\text{min}_\theta\ \text{max}_\psi\ \mathbb{E}_{y\sim p_{\text{data}}(y|x)}[\log D_\psi(y)] + \mathbb{E}_{y\sim p_{\text{model}}(y|x)}[\log \left( 1-D_\psi(y) \right) ]\\
\end{align}

Assuming: $y = f(x,W), x \sim p(x) \iff y \sim p(y|x)$, we can write this as \footnote{Really not sure of this assumption! Even if it is true, how do you compute these gradients $\frac{\partial}{\partial W} \mathbb{E}_{x\sim p(x)}[h(f(x,W))]$}?,

\begin{align}
\text{min}_\theta\ \text{max}_\psi\ \mathbb{E}_{y\sim p_{\text{data}}(y|x)}[\log D_\psi(y)] + \mathbb{E}_{x\sim p(x)}[\log \left( 1-D_\psi(f(x,W)) \right) ]\\
\end{align}

Now of course, we would need gradients of this objective function with respect to $W$ for learning $f$. 

\paragraph{Learning with posterior. }
Now if a posterior is learnt over these weights, $q(W)$, one can succesfully obtain the required samples in Equaion~\eqref{eq:gan-obj}. This is under the folllowing assumption $y = f(x,W), W \sim q(W) \iff y \sim p(y|x)$. \footnote{Is this consistent with $p_{\text{model}}(y|x) = \int p(y|x,W)q(W) dW$?}. During training, we need gradients of the form, $\frac{\partial}{\partial W} \mathbb{E}_{W\sim q(W)}[D_\psi(f(x,W))]$. Using the reparametrisation trick, we can compute these as, $\mathbb{E}_{\epsilon \sim p(\epsilon)}\left[\frac{\partial}{\partial W} D_\psi(f(x,t_W(\epsilon)))\right]$

Suppose we have a model $t_\theta$ which defines the approximate posterior, $q(W)$, i.e. if $\{\epsilon_i\}_{i=1}^M \sim p(\epsilon)$, then \{$W_i | W_i = t_\theta(\epsilon_i)\}_{i=1}^M\sim q(W)$. Subsequently, one could obtain a sample $y_i \sim p_{\text{approx}}(y|x)$ as $f(x,t_\theta(\epsilon_i))$.


\footnote{For any function $h$, is this true? (I am struggling to see the equivalence between the first two terms) - $\mathbb{E}_{y\sim p_{\text{model}}(y|x)}[h(y)] = \mathbb{E}_{W\sim q(W)}[h(f(x,W))] = \mathbb{E}_{\epsilon\sim p_{\epsilon}}[h(f(x,t_\theta(\epsilon)))]$}


The ``desired'' second term needs to be reformulated in temrs of $p(\epsilon)$. In short any expectation over $p_{\text{approx}}(y|x)$ needs to be written as an expectation over $p(\epsilon)$), but are they equivalent? or can we sample moments of $W$ and then integrate under some gaussian assumptions?

\begin{align}
\mathbb{E}_{\epsilon\sim p_{\epsilon}}[\log \left( 1-D_\psi(f(t_\theta(\epsilon))) \right) ]\\
\end{align}


The problem is that while computing $y_i$ as $f(x,t_\theta(\epsilon_i))$, we are not marginalising over the weights. 
% It could take as input a vector $x$ along with a set of noise-samples $\{\epsilon_i\}_{i=1}^M \sim p(\epsilon)$ and outputs samples $y \sim p_{\text{approx}}(y|x)$. This can be thought of as learning a prametric model $t_\theta$ for the approximate posterior,

\begin{align}
p_{\text{approx}}(y|x) &= \int p(y|x,W)q(W) dW\\
& = \int p(y|x,t_\theta(\epsilon))p(\epsilon)d\epsilon\\
&\approx \frac{1}{M} \sum_{m=1}^{M} p(y|x,t_\theta(\epsilon_m)), \ \epsilon \sim p(\epsilon)
\end{align}

To make this more clear, for every $\epsilon_i$ what you get is a paired-sample $(y_i,W_i)$ from the joint distribution $p(y_i.W_i|x)$. Thus, marginalisation with respect to $W$ is a must here. Otherwise, your approximation model for posterior is a dirac on $W_i$.


\section{ELBO - I}

Suppose there is a parametric model $g_\theta$ which takes as input noise and outputs the mean and variance corresponding to the approximate posterior. Now if you make the same assumption as PBP, that the output of the network admit a Gaussian distribution then you can sucessfully represent a sample from the distribution $p(y|x)$. One can then learn this model using either the ELBO, or the GAN-way.

Assumtions: It is assumed that the approximate posterior factors over weight, conditioned on the latent space, which is more complex then a naively factored distribution of weights. 

\begin{align}
q(W) = \prod_{j=1}^Lq(W_j) = \prod_{j=1}^L \mathcal{N}(m_j,v_j)
\end{align}

\begin{align}
q(W) &= \int q(W,z)dz\\
&= \int q(W|z)p(z)dz\\
&= \int \left(\prod_{j=1}^Lq(W_j|z)\right)p(z)dz\\
&= \int \left(\prod_{j=1}^L\mathcal{N}(m_j,v_j)\right)\mathcal{N}(m_z,v_z)dz\\
\end{align}

This $q$ is tractable. (\textbf{TODO}: integrate out)

\begin{align}
\epsilon_i \sim p(\epsilon) \rightarrow \{m_{i,j},v_{i,j}\}_{j=1}^{L} = t_\theta(\epsilon_i) \rightarrow q\\
\end{align}

Limitations: 1. the approximate posterior in this model is not as expressive as hypernetworks'. 2. This wouldn't scale to high dimensional datasets if you make such big assumptions like normally distributed outputs. 

\section{ELBO - II}

This idea attempts to learm a latent space of the approximate posterior. I will discuss how this differs from Hypernetworks and what's the advantage of this modelling. The merits are clear if you see the generative process for getting samples of $q(W)$ differs in the two approaches. 

\begin{align}
\epsilon_i \sim p(\epsilon) \rightarrow W_i = t_\theta(\epsilon_i)\\
m,v \rightarrow z_i \sim \mathcal{N}(m,v) \rightarrow W_i = t_\theta(z_i)
\end{align}

The previious approach tried to factor $q$ conditioned on $z$. Instead, one can assume a non-parametric distribution for $q(W|z)$. So alternatively,


\begin{align}
q(W) &= \int q(W,z)dz\\
&= \int q(W|z)p(z)dz\\
\end{align}

\begin{itemize}
  \item Is this equivalent to putting hierarchical priors in the Hypernetwork formulation? 
\end{itemize}

Both the formilations I and II can be interpretted as learning a latent space for approximate posterior. 

 
\section{Learning a parametric model for transition distributions in MCMC}

In a Markov Chain MC sampler, there is a ditribution $T(W^{t}|W^{t-1})$ which captures the transitions. The stationary distribution of this chain corresponds to the required distribution for which the sampler is developed which in our case is the approximate posterior $q(W)$. 

\begin{align}
q(W^t) = \int T(W^{t}|W^{t-1})q(W^{t-1})dW
\end{align}

Is it possible to learn a NN-model for this transition distribution? Something similar to ``Neural Adaptive Sequential Monte Carlo [Gu et. al. 2014]''


% section distribution_perspective (end)

%\bibliographystyle{unsrt}
%\bibliography{citations}
% section siamese_networks (end)
\end{document}



